<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TPO</title>
  <link rel="icon" type="image/x-icon" href="static/images/project_icon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <style> .image_container { display: flex; justify-content: space-between; } </style>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">

            <!-- Title -->
            <h1 class="title is-1 publication-title">A Topic-level Self-Correctional Approach to Mitigate Hallucinations in MLLMs</h1>
            
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="" target="_blank">Lehan He</a><sup>*1,2,3</sup>
              </span>,
              <span class="author-block">
                <a href="" target="_blank">Zeren Chen</a><sup>*1,2</sup>
              </span>,
              <span class="author-block">
                <a href="" target="_blank">Zhelun Shi</a><sup>1,2</sup>
              </span>,
              <span class="author-block">
                <a href="" target="_blank">Tianyu Yu</a><sup>4</sup>
              </span>
              <br>
              <span class="author-block">
                <a href="" target="_blank">Jing Shao</a><sup>â€ 1</sup>
              </span>,
              <span class="author-block">
                <a href="" target="_blank">Lu Sheng</a><sup>â€ 2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Shanghai AI Laboratory
                ,
                <sup>2</sup>School of Software, Beihang University
                <br>
                <sup>3</sup>Shanghai Innovation Institute
                ,
                <sup>4</sup>Tsinghua University
              </span>
              <span class="eql-cntrb">
                <small>
                  <br>
                  <sup>*</sup>Indicates Equal Contribution
                  ,
                  <sup>â€ </sup>Corresponding authors
                </small>
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">

                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.17265" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                
                <!-- Dataset link -->
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

                <!-- Model link -->
                <span class="link-block">
                  <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-comment"></i>
                    </span>
                    <span>Model</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- project introduce -->
  <!-- 
  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
      <font size="5">
        <div class="has-text-centered">
          <p>ðŸ”¥<b>What's New</b></p>
        </div>
      </font>
      <font size="4">
        <table width="75%" align="center" border="1" cellspacing="0" cellpadding="10"><tbody>
          <tr>
            <td>
              <div style="height: 55px; overflow: auto;">
                <ul style="list-style-type: none; padding: 0;">
                  <li style="display: inline-block; width: 100%;"> <span style="font-family: monospace;">[2024.11.27]</span> ðŸš€ We released the ArXiv paper.</li>
                  <li style="display: inline-block; width: 100%;"> <span style="font-family: monospace;">[2024.12.XX]</span> ðŸš€ Code, model and data will be released soon.</li>
                </ul>
              </div>
            </td>
          </tr>
        </tbody>
      </table>
    </font>
    </div>
  </section>
  -->

  <!-- project introduce 
  <section class="hero teaser is-light" id="abstract">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <br>
          <br>
          <h2 class="title is-3">Background and Motivation</h2>
          <div class="content has-text-justified">
            <p>
              Existing MLLMs continue to face challenges related to hallucinations.
              Recent attempts have employed human experts or powerful auxiliary AI systems to provide more accurate preference feedback.
              However, the responses of MLLMs are usually long, complex and ambiguous with inevitable flaws, 
              which interferes the preference optimization due to the remaining hallucinations in the preferred responses.
              <br>
              <br>
              An intuitive alternative is to enhance the quality of preference pairs by directly correcting or contaminating the original responses.
              Some approaches (See) rely on extensive human annotations or ultra-large proprietary models 
              (such as GPT-4V) to detect hallucinations and then rewrite the responses, therefore the scalability of feedback data is still limited.
              <br>
            </p>
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="100%" src="static/images/motivation.png">     
              </div>
            </centering>
            <p>
              <br>
              To address this issue, we propose leveraging the reference model itself to enhance the preference pairs in a self-correctional manner, 
              without human or proprietary model intervention.
            </p>        
            <br>
            <br>
          </div>
        </div>
      </div>
    </div>
  </section>
  End project introduce -->



  <!-- method -->
  <section class="hero is-small">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3 mathvista">
        <span class="mathvista" style="vertical-align: middle">Background and Motivation</span>
      </h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              <div class="content has-text-justified">
                <p>
                  Existing MLLMs continue to face challenges related to hallucinations.
                  Recent attempts have employed human experts or powerful auxiliary AI systems to provide more accurate preference feedback.
                  However, the responses of MLLMs are usually long, complex and ambiguous with inevitable flaws, 
                  which interferes the preference optimization due to the remaining hallucinations in the preferred responses.
                  <br>
                  <br>
                  An intuitive alternative is to enhance the quality of preference pairs by directly correcting or contaminating the original responses.
                  Some approaches rely on extensive human annotations or ultra-large proprietary models 
                  (such as GPT-4V) to detect hallucinations and then rewrite the responses, therefore the scalability of feedback data is still limited.
                  <br>
                </p>
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/motivation_nocaption.png">     
                  </div>
                </centering>
                <b><span style="font-size: 90%;">(a) Conventional RLAIF baselines generate feedback by using labeler models to distinguish preferences, leading to sub-optimal results.</span></b>
                <b><span style="font-size: 90%;">(b) Methods that rely on extensive manual annotation or proprietary models for feedback collection, compromising the scalability of feedback data.</span></b>
                <b><span style="font-size: 90%;">(c) We propose a topic-level self-correctional paradigm tailored for reducing hallucinations, through topic clustering and topic overwriting.</span></b>
                <br>
                <p>
                  To address this issue, we propose leveraging the reference model itself to enhance the preference pairs in a self-correctional manner, 
                  without human or proprietary model intervention.
                </p>
                <br>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End method -->


  <!-- method -->
  <section class="hero is-small">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3 mathvista">
        <img src="static/images/project_icon.ico" style="width:1em;vertical-align: middle" alt="Logo"/>
        <span class="mathvista" style="vertical-align: middle">Topic-level Preference Overwriting</span>
      </h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              <!-- <p>
                The responses of MLLMs are usually long, complex and ambiguous with inevitable flaws, 
                which interferes the preference optimization due to the remaining hallucinations in the preferred responses.
                
              </p>
              <centering>
                <div style="text-align: center;">
                  <img id="teaser" width="100%" src="static/images/motivation.png">     
                </div>
              </centering>
              <br>
              <br>
              <p>
                <strong>The proposed TPO framework:</strong>
              </p> 
              <p>
                We collect 21k feedback data by using TPO pipeline to correct or contaminate the model responses.
                The training takes only 1 hour with 8 A100 GPUs to get our TPO-7B model which is initialized from <a href="https://huggingface.co/liuhaotian/llava-v1.5-7b">LLaVA-1.5-7B</a>.
              </p> -->
              <p>
                We propose a topic-level self-correctional paradigm tailored for reducing hallucinations, 
                <b>T</b>opic-level <b>P</b>reference <b>O</b>verwriting (<b>TPO</b>).
                We adopt a deconfounded algorithm that replaces all topics involved in a complex response, 
                with the best or worst alternatives resampled multiple times from the reference model itself on the same topic.
              </p>
              <centering>
                <div style="text-align: center;">
                  <img id="teaser" width="100%" src="static/images/overall_paradigm_nocaption.png">     
                </div>
              </centering>
              <b><span style="font-size: 90%;">(1) Decomposing multiple responses generated by the reference model into sub-responses and resampling additional candidate sub-responses with wh-questions.(<em>e.g.</em>, what, where, how)</span></b>
              <b><span style="font-size: 90%;">(2) Clustering all sub-responses into several distinct topics based on textual and visual semantics.</span></b>
              <b><span style="font-size: 90%;">(3) Scoring the sub-responses under each topic and selecting the sub-response with the highest and lowest score to construct topic-level preference pair for each topic.</span></b>
              <b><span style="font-size: 90%;">(4) Correcting the response by overwriting its sub-responses with topic-level preferences.</span></b>
              <b><span style="font-size: 90%;">(5) The reference model is fine-tuned with feedback data through DPO.</span></b>
              <br>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End method -->


  <!-- performance -->
  <section class="hero is-small">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3 mathvista">
        <span class="mathvista" style="vertical-align: middle">ðŸ“ƒ Highlights</span>
      </h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              <p>
                Without bells and whistles, 
                TPO achieves state-of-the-art performance in trustworthiness across several hallucination benchmarks, 
                reducing hallucination of the base model by ~92% on ObjectHal-Bench, 
                and by ~38% on MMHal-Bench.
                We also align base model with the model itself as labeler, 
                significantly reducing its own hallucinations (by ~88% on ObjectHal-Bench and by ~12% on MMHal-Bench) 
                and breaking through its inherent limitations.
              </p>
              <centering>
                <div style="text-align: center;">
                  <img id="teaser" width="100%" src="static/images/main_result.png">     
                </div>
              </centering>
              <br>
              <br>
              <p>
                TPO allows us to collect more feedback data for hallucination reduction at a low cost, without human or proprietary models intervention.
                As the data scale increases, the trustworthiness of the model continuously improves.
              </p>
              <centering>
                <div style="text-align: center;">
                  <img width="60%" src="static/images/data_scale_nocaption.png" alt="Data Scale Image"> 
                </div>
                <!-- 
                <div class="image_container">
                  <img width="50%" src="static/images/data_scale.png" alt="Data Scale Image"> 
                  <img width="45%" src="static/images/win_rate.png" alt="Win Rate Image">
                </div> -->
              </centering>
              <br>
              <p>
                We compare the quality of preferred responses generated by TPO with those identified by the labeler model and original responses.
                We evaluate their informativeness and trustworthiness based on GPT-4V evaluation review.
                Different colors in the pie charts mark the number of winning responses.
                TPO outperforms its counterparts in both informativeness and trustworthiness.
              </p>
              <centering>
                <div style="text-align: center;">
                  <img width="50%" src="static/images/win_rate.png" alt="Win Rate Image">
                </div>
              </centering>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End performance -->


  <!-- example -->
  <section class="hero is-small">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3 mathvista">
        <span class="mathvista" style="vertical-align: middle">ðŸ–Œ Examples</span>
      </h2>
    </div>
    <div class="hero-body">
      <div class="container">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content has-text-justified">
              <p>
                <span style="color:#00b050;"><b>Correct answers</b></span> 
                and 
                <span style="font-weight:bold; color:#c00000;"><b>hallucinations</b></span> 
                are highlighted in color respectively.
              </p>
              <centering>
                <div style="text-align: center;">
                  <img width="60%" src="static/images/more_case_study_data_page1_00.png">
                </div>
              </centering>
              <centering>
                <div style="text-align: center;">
                  <img width="60%" src="static/images/more_case_study_data_page2_00.png">
                </div>
              </centering>
              <centering>
                <div style="text-align: center;">
                  <img width="60%" src="static/images/more_case_study_model_page1_00.png">
                </div>
              </centering>
              <centering>
                <div style="text-align: center;">
                  <img width="60%" src="static/images/more_case_study_model_page2_00.png">
                </div>
              </centering>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End example -->


  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{he2024topic,
          title={A Topic-level Self-Correctional Approach to Mitigate Hallucinations in MLLMs}, 
          author={Lehan He and Zeren Chen and Zhelun Shi and Tianyu Yu and Jing Shao and Lu Sheng},
          journal={arXiv preprint arXiv:2411.17265},
          year={2024}
        }
      </code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the 
                <a href="https://nerfies.github.io" target="_blank">
                  Nerfies
                </a> 
                project page, licensed under a 
                <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">
                  Creative Commons Attribution-ShareAlike 4.0 International License
                </a>
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>


</body>

</html>
